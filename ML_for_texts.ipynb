{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPDN6H3Wt+HcjqpbtMzY4M7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakhayeva/Data-Science-Projects-in-English/blob/main/ML_for_texts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project for \"Wikishop\" with BERT"
      ],
      "metadata": {
        "id": "omNo7d1FfGuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Wikishop\" online store is launching a new service. Users can now edit and supplement product descriptions, similar to wiki communities. In other words, customers suggest edits and comment on the changes made by others. The store needs a tool that will detect toxic comments and send them for moderation.\n",
        "\n",
        "**Objective**: Train a model to classify comments into positive and negative. You have a dataset at your disposal with labels indicating the toxicity of the edits.\n",
        "\n",
        "**Requirement**: Build a model with an $F1$ quality metric of at least **0.75**.\n",
        "\n",
        "**Project Instructions**\n",
        "- [Downloading and prepare the data](#Downloading).\n",
        "- [Training different models](#Training).\n",
        "- [Drawing conclusions](#conclusions).\n"
      ],
      "metadata": {
        "id": "yG4vK4OmfGlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Description"
      ],
      "metadata": {
        "id": "SC2J557VfkFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is stored in the `toxic_comments.csv` file.\n",
        "- The text column contains the comment text.\n",
        "- The toxic column is the target feature."
      ],
      "metadata": {
        "id": "cT3fH5Eqfmof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name='Downloading'></a> Downloading and Preparing Data"
      ],
      "metadata": {
        "id": "WEKjYX27npmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modules and Libraries Import"
      ],
      "metadata": {
        "id": "Rb7HkZZUndo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost -q"
      ],
      "metadata": {
        "id": "cnPVaJRNd7ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "VyQF9_79dbzT",
        "outputId": "4f788c57-ac7d-4bd5-c5ff-6782035a6a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from tqdm import notebook\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import f1_score\n",
        "from catboost import CatBoostClassifier\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import output\n",
        "# Preventing disconnect in Colab environment\n",
        "output.eval_js('function ClickConnect(){console.log(\"Preventing disconnect\");document.querySelector(\"colab-toolbar-button#connect\").click()}setInterval(ClickConnect,60000)')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name='Loading'></a> Data Loading"
      ],
      "metadata": {
        "id": "2RtKbLqOdrS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Yandex_Practicum/datasets for DS/toxic_comments.csv')"
      ],
      "metadata": {
        "id": "rGiuoDFidng9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the class distribution\n",
        "class_distribution = df['toxic'].value_counts(normalize=True)\n",
        "class_distribution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "TL9lbW6JgSmx",
        "outputId": "2c3b58f1-23ce-4121-8d38-e2f57a5a07e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "toxic\n",
              "0    0.898388\n",
              "1    0.101612\n",
              "Name: proportion, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>proportion</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>toxic</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.898388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.101612</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a significant class imbalance. We should use balancing methods when training the LogisticRegression model."
      ],
      "metadata": {
        "id": "q5LlpgXLgbZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering with BERT"
      ],
      "metadata": {
        "id": "F-3D2_T-ggXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample a portion of the data due to limited computational power for the full dataset\n",
        "df_sample = df.sample(1000).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Jqxb7w7Egf0C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary characters (cleaning)\n",
        "df_sample['text'] = df_sample['text'].replace(to_replace='[^\\w\\s]', value='', regex=True)\n",
        "\n",
        "# Loading pretrained tokenizer and model\n",
        "model_name = 'unitary/toxic-bert'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Prepare device for training (GPU support)\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Tokenizing texts\n",
        "tokenized = df_sample['text'].apply(\n",
        "    lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=512))\n",
        "\n",
        "# Determine the maximum length among all tokenized sequences\n",
        "max_len = max(len(seq) for seq in tokenized)\n",
        "\n",
        "# Pad all sequences to the same length (max_len) with zeros\n",
        "padded = np.array([seq + [0] * (max_len - len(seq)) for seq in tokenized])\n",
        "\n",
        "# Create attention masks: 1 for real tokens, 0 for padding tokens\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "\n",
        "# Set batch size\n",
        "batch_size = 100\n",
        "embeddings = []\n",
        "\n",
        "# Move model to GPU\n",
        "model.to(device)\n",
        "\n",
        "# Disable gradient calculation (not required for inference/embedding extraction)\n",
        "with torch.no_grad():\n",
        "    # Iterate over the data in batches\n",
        "    for i in notebook.tqdm(range(0, len(padded), batch_size)):\n",
        "        # Create data batch and attention mask batch\n",
        "        batch = torch.LongTensor(padded[i:i + batch_size])\n",
        "        attention_mask_batch = torch.LongTensor(attention_mask[i:i + batch_size])\n",
        "\n",
        "        # Get model outputs\n",
        "        outputs = model(batch.to(device), # Move data to GPU\n",
        "                        attention_mask=attention_mask_batch.to(device))\n",
        "\n",
        "        # Extract CLS token embeddings and move them back to CPU as numpy arrays\n",
        "        embeddings.append(outputs.last_hidden_state[:, 0, :].cpu().numpy())\n",
        "\n",
        "# Concatenate all embeddings into a single feature matrix\n",
        "features = np.concatenate(embeddings)"
      ],
      "metadata": {
        "id": "x5ZKbwQvgZAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the shape of the resulting embeddings array\n",
        "display(features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "r2dfHn19mi42",
        "outputId": "c4bf6853-0be3-4fa8-f6c6-56a3d54306ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(1000, 768)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name='Training'></a> Training Models"
      ],
      "metadata": {
        "id": "R5zGjU1chWuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LinearRegression"
      ],
      "metadata": {
        "id": "V4XHXr9MhbUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of LogisticRegression\n",
        "# We use class_weight='balanced' to handle the identified class imbalance\n",
        "lr = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "\n",
        "# Define the hyperparameter grid for search\n",
        "param_grid_lr = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'max_iter': [500, 1000]\n",
        "}\n",
        "\n",
        "# Configure GridSearchCV\n",
        "grid_lr = GridSearchCV(lr, param_grid_lr, scoring='f1', cv=5)\n",
        "\n",
        "# Search for the best hyperparameters\n",
        "grid_lr.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Extract the model with the best parameters\n",
        "best_lr = grid_lr.best_estimator_\n",
        "\n",
        "print(f'Best parameters for LogisticRegression: {grid_lr.best_params_}')\n",
        "print(f'F1 via cross-validation: {grid_lr.best_score_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FNGJOiMhYdM",
        "outputId": "d631e757-03f6-451a-b30e-dfea7b4570e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for LogisticRegression: {'C': 10, 'max_iter': 500}\n",
            "F1 via cross-validation: 0.9034159166750936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CatBoost"
      ],
      "metadata": {
        "id": "SJ9S6X_Xiwnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of CatBoostClassifier\n",
        "# Using GPU for faster training\n",
        "cb = CatBoostClassifier(random_state=42, verbose=0, task_type='GPU', devices='0')\n",
        "\n",
        "# Define the hyperparameter grid for search\n",
        "param_grid_cb = {\n",
        "    'iterations': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'depth': [4, 6, 8]\n",
        "}\n",
        "\n",
        "# Configure GridSearchCV\n",
        "grid_cb = GridSearchCV(cb, param_grid_cb, scoring='f1', cv=5)\n",
        "\n",
        "# Search for the best hyperparameters\n",
        "grid_cb.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Extract the model with the best parameters\n",
        "best_cb = grid_cb.best_estimator_\n",
        "\n",
        "print(f'Best parameters for CatBoost: {grid_cb.best_params_}')\n",
        "print(f'F1 via cross-validation: {grid_cb.best_score_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpaXtuC5iuBu",
        "outputId": "2526cf30-1afb-459f-bc93-c70ae50f5d83"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for CatBoost: {'depth': 8, 'iterations': 150, 'learning_rate': 0.01}\n",
            "F1 via cross-validation: 0.9005561735261403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best Model Selection and Testing"
      ],
      "metadata": {
        "id": "Qe-_X_bni36m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best model based on cross-validation results\n",
        "if grid_lr.best_score_ > grid_cb.best_score_:\n",
        "    final_model = best_lr\n",
        "    model_name = \"LogisticRegression\"\n",
        "else:\n",
        "    final_model = best_cb\n",
        "    model_name = \"CatBoost\"\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred_final = final_model.predict(X_test)\n",
        "f1_final = f1_score(y_test, y_pred_final)\n",
        "\n",
        "print(f'Best model: {model_name}')\n",
        "print(f'Test F1 score: {f1_final}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju4Vz0lXi0vT",
        "outputId": "af1f3e80-49f0-4ca4-b50d-990e1ab40f7b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: LogisticRegression\n",
            "Test F1 score: 0.8936170212765957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name='conclusions'></a> Conclusions"
      ],
      "metadata": {
        "id": "nLBxP64rk-JO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As part of the project for the \"Wikishop\" online store, we were tasked with developing a machine learning model to automatically detect toxic comments and flag them for moderation. The primary objective was to train a classifier to distinguish between positive and negative comments, achieving an $F1$ score of at least **0.75**.\n",
        "\n",
        "**Project Workflow:**\n",
        "- **Data Preprocessing:**\n",
        "   - An analysis of the class distribution revealed a significant imbalance: toxic comments accounted for only about 10% of the dataset.\n",
        "   - To optimize computational efficiency, a random sub-sample of 800 records was selected for the initial experiment.\n",
        "   - Text data was cleaned of unnecessary characters and noise using regular expressions.\n",
        "- **Modeling and Feature Engineering:**\n",
        "- We explored two classification algorithms: `Logistic Regression` and `CatBoost`.\n",
        "- Both models utilized high-dimensional text embeddings extracted via the pre-trained `unitary/toxic-bert` model.\n",
        "- To mitigate the class imbalance, we applied the `class_weight='balanced'` parameter for Logistic Regression and utilized CatBoostâ€™s internal imbalance-handling mechanisms.\n",
        "- Hyperparameter tuning and performance estimation were conducted using **GridSearchCV** with 5-fold cross-validation.\n",
        "\n",
        "**Performance Evaluation:**\n",
        "- **Cross-Validation Results:**\n",
        "   - Logistic Regression Parameters: `{'C': 10, 'max_iter': 500}`. Achieved an $F1$ score of 0.9.\n",
        "   - CatBoost Parameters: `{'depth': 8, 'iterations': 150, 'learning_rate': 0.01}`. Also demonstrated a high $F1$ score of 0.9.\n",
        "- **Final Test Results:**\n",
        "The **Logistic Regression** model achieved an $F1$ score of **0.89** on the test set. This result is consistent with the validation phase, indicating that the model generalizes well to unseen data without significant overfitting."
      ],
      "metadata": {
        "id": "s9pCfbRIlEbS"
      }
    }
  ]
}